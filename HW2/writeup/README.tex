\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage[letterpaper, portrait, margin=1in]{geometry}
\usepackage{amsmath}

\title{Natural Language Processing\\Assignment 2: Probability and Vector Exercises}
\author{Li-Yi Lin, JHED ID: 5F0CE8}
\date{}

\begin{document}
\maketitle

\noindent
\textbf{Question 1.}\\
\textbf{(a) Answer:}\\
Since $Y \subseteq Z$, we have $Z = Y \cup (Z \cap \neg Y)$. So the probability of Z is
\begin{align*}
  p(Z)
  &= p(Y \cup (Z \cap \neg Y))
  \\ &= p(Y) + p(\emptyset)                   
  \\ &\geq p(Y) + 0         \tag{$p(\emptyset) \geq 0$}
  \\ &= p(Y)  
\end{align*}
Thus, we proved that if $Y \subseteq Z$, then $p(Y) \leq p(Z)$.
\\

\noindent
\textbf{(b) Answer:}\\
We know that $p(X \mid Z) = \frac{p(X \cap Z)}{p(Z)}$. Since $(X \cap Z) \subseteq Z$, we have the fact that $p(X \cap Z) \leq p(Z)$. Therefore, $p(X \mid Z) = \frac{p(X \cap Z)}{p(Z)} \leq 1$. Moreover, since probability is larger than or equal to 0, we have $p(X \cap Z) \geq 0$. Because $p(X \mid Z)$ is conditioned on $Z$, we have $p(Z) > 0$. Therefore, by the property that $0 \leq p(X \cap Z) \leq p(Z)$, we can prove that $p(X \mid Z)$ always fall in the range [0,1].
\\

\noindent
\textbf{(c) Answer:}\\
Since $E \cap \emptyset = \emptyset$, we have $p(E \cup \emptyset) = p(E) + p(\emptyset)$ by the given axioms. Because we know that $p(E) = 1$ and probability cannot be larger than 1, $p(E \cup \emptyset) = p(E) + p(\emptyset) = 1 + p(\emptyset) \leq 1$. Therefore $p(\emptyset)$ must be 0.
\\

\noindent
\textbf{(d) Answer:}\\
Let $\bar{X}$ denote $E - X$. We have $\bar{X} \cup X = E$. Now we are going to prove that $p(X) = 1 - p(\bar{X})$.\\
Since $\bar{X} \cap X = \emptyset$ and $\bar{X} \cup X = E$, we have
\begin{align*}
  p(E)
  &= p(\bar{X} \cup X)
  \\ &= p(\bar{X}) + p(X) - p(\bar{X} \cap X)          
  \\ &= p(\bar{X}) + p(X)       \tag{$p(\bar{X} \cap X) = 0$ by (c)}
\end{align*}
By the equation above, we have $p(E) = p(\bar{X}) + p(X)$. Since $p(E) = 1$, we have $1 = p(\bar{X}) + p(X)$. Therefore we have proven that $p(X) = 1 - P(\bar{X})$.
\\

\noindent
\textbf{(e) Answer:}\\
\begin{align*}
  p(singing \cap rainy \mid rainy)
  &= \frac{p((singing \cap rainy) \cap rainy)}{p(rainy)}
  \\&= \frac{p(singing \cap (rainy \cap rainy))}{p(rainy)}  \tag{Intersection is an associative operation}
  \\ &= \frac{p(singing \cap rainy)}{p(rainy)}  \tag{$rainy \cap rainy = rainy$}    
  \\ &= p(singing \mid rainy)      
\end{align*}
By the equation above, we proved that $p(singing \text{ AND } rainy \mid rainy) = p(singing \mid rainy)$.
\\

\noindent
\textbf{(f) Answer:}\\
\begin{align*}
  p(Y)
  &= p((X \cap Y) \cup (\bar{X} \cap Y))
  \\&= p(X \cap Y) + p(\bar{X} \cap Y) - p((X \cap Y) \cap (\bar{X} \cap Y))  
  \\ &= p(X \cap Y) + p(\bar{X} \cap Y)     \tag{$(X \cap Y) \cap (\bar{X} \cap Y) = \emptyset$}
\end{align*}\\
By dividing both sides by $p(Y)$, we proved that\\
$$1 = \frac{p(X \cap Y)}{p(Y)} + \frac{p(\bar{X} \cap Y)}{p(Y)}$$
$$p(X \mid Y) = 1 - p(\bar{X} \mid Y)$$
\\

\noindent
\textbf{(g) Answer:}\\
$$(p(X \mid Y)\cdot p(Y) + p(X \mid \bar{Y}) \cdot p(\bar{Y})) \cdot p(\bar{Z} \mid X) / p(\bar{Z})$$
%
$$= (\frac{p(X \cap Y)}{p(Y)} \cdot p(Y) + \frac{p(X \cap \bar{Y})}{p(\bar{Y})} \cdot p(\bar{Y})) \cdot \frac{p(\bar{Z} \cap X)}{p(X)}/p(\bar{Z})$$
%
$$= (p(X \cap Y) + p(X \cap \bar{Y})) \cdot \frac{p(\bar{Z} \cap X)}{p(X)}/p(\bar{Z})$$
%
$$= p(X) \cdot \frac{p(\bar{Z} \cap X)}{p(X)}/p(\bar{Z})$$
%
$$= \frac{p(\bar{Z} \cap X)}{p(\bar{Z})}$$
% 
$$= p(X \mid \bar{Z})$$
\\

\noindent
\textbf{(h) Answer:}\\
We know that 
$$p(singing \text{ AND } rainy) = p(singing) + p(rainy) - p(singing \cap rainy)$$
So, when $(singing \cap rainy) = \emptyset$, then  
\begin{align*}
  p(singing \text{ AND } rainy)
  &= p(singing) + p(rainy) - p(singing \cap rainy)
  \\&= p(singing) + p(rainy) - p(\emptyset)
  \\ &= p(singing) + p(rainy) - 0     \tag{by (c)}
  \\ &= p(singing) + p(rainy)
\end{align*}\\
Therefore, when set $singing$ and $rainy$ are disjoint, the given equation is true.\\

\noindent
\textbf{(i) Answer:}\\
Assume the given equation is true, then
$$p(singing \text{ AND } rainy) = p(singing) \cdot p(rainy)$$
We further assume that $p(rainy) > 0$ and divide the above equation by $p(rainy)$
$$\frac{p(singing \text{ AND } rainy)}{p(rainy)} = p(singing)$$
$$p(singing \mid rainy) = p(singing)$$
Therefore, for the $p(singing \text{ AND } rainy) = p(singing) \cdot p(rainy)$ to be true, $p(singing \mid rainy) = p(singing)$ must also be true. It means that the probability event $rainy$ will not affect the probability of event $singing$.\\

\noindent
\textbf{(j) Answer:}\\
We are given that
$$p(X \mid Y) = 0$$
$$\frac{p(X \cap Y)}{p(Y)} = 0$$
For the above equation to be 0, $p(X \cap Y)$ must be 0.\\

\noindent
Then, we are going to prove that $p(X \mid Y, Z) = 0$
\begin{align*}
  p(X \mid Y, Z)
  &= \frac{p(X \cap Y \cap Z)}{p(Y \cap Z)}
  \\&\leq \frac{p(X \cap Y)}{p(Y \cap Z)}           \tag{by (a), $(X \cap Y \cap Z) \subseteq (X \cap Y)$, so $p(X \cap Y \cap Z) \leq p(X \cap Y)$}
  \\&= \frac{0}{p(Y \cap Z)}
  \\&= 0
\end{align*}\\
Thus, we have proved that if $p(X \mid Y) = 0$, then $p(X \mid Y, Z) = 0$.\\

\noindent
\textbf{(k) Answer:}\\
By (f), we know 
\begin{align*}
  p(\bar{W} \mid Y)
  &= 1 - p(W \mid Y)
  \\&= 1 - 1 
  \\&= 0
\end{align*}\\
Since $p(\bar{W} \mid Y) = \frac{p(\bar{W} \cap Y)}{p(Y)} = 0$, then $p(\bar{W} \mid Y)$ must be 0.\\ 

\noindent
Then, we are going to prove that $p(W \mid Y, Z) = 1$
\begin{align*}
  p(W \mid Y, Z)
  &= 1 - p(\bar{W} \mid Y, Z)       \tag{by (f)}
  \\&= 1 - \frac{p(\bar{W} \cap Y \cap Z)}{p(Y \cap Z)}
  \\&= 1 - 0        \tag{since $(\bar{W} \cap Y \cap Z) \subseteq (\bar{W} \cap Y), p(\bar{W} \cap Y \cap Z) \leq p(\bar{W} \cap Y)$ by (a)}
  \\&= 1
\end{align*}\\
Thus, we have proven that if $p(W \mid Y) = 1$, then $p(W \mid Y, Z) = 1$.\\

\newpage
\noindent
\textbf{Question 2.}
\textbf{(a) Answer:}\\
Denote event "$Actual = Blue$" as $A$, "$Claimed = blue$" as $C$.\\
By Bayes' theorem, we have 
$$p(A \mid C)p(C) = p(C \mid A)p(A)$$\\

\noindent
\textbf{(b) Answer:}\\
Since $p(Actual = blue) = 0.1$ is fixed. We further rewrite the equation in (a) as shown below:
$$p(Actual = blue \mid Claimed = blue) \propto p(Claimed = blue \mid Actual = blue)p(Actual = blue)$$
Then, the prior probability is $p(Actual = blue)$,\\
the likelihood of the evidence is $p(Claimed = blue \mid Actual = blue)$,\\ and the posterior probability is $p(Actual = blue \mid Claimed = blue)$.\\

\noindent
\textbf{(c) Answer:}\\
prior probability = $p(Actual = blue) = 0.1$\\
likelihood of the evidence = $p(Claimed = blue \mid Actual = blue) = 0.8$\\
posterior probability = $\frac{p(Claimed = blue \mid Actual = blue) \cdot p(Actual = blue)}{p(Claimed = blue)} = \frac{0.8 \times 0.1}{p(Claimed = blue)}$\\

\noindent
Now we need to calculate $p(Claimed = blue)$.\\
$p(Claimed = blue)$\\
$= p(Claimed = blue \mid Actual = blue) \cdot p(Actual = blue) + p(Claimed = blue \mid Actual = red) \cdot p(Actual = red)$\\
$= 0.8 \times 0.1 + 0.2 \times 0.9$\\
$= 0.26$\\

\noindent
So, posterior probability = $\frac{0.8 \times 0.1}{p(Claimed = blue)} = \frac{0.08}{0.26} = 0.3077$\\

\noindent
The judge should care about the posterior because it takes not only about the observed events, likelihood, but also the belief about the probability of the event we want to guess. By considering the posterior probability, the judge can avoid the situation that our belief is wrong or we have not enough observed events so that the judge can have more accurate information for judgement.\\

\noindent
\textbf{(d) Answer:}\\
To prove the given equation, we start with the right hand side:
\begin{align*}
  \frac{p(B \mid A, Y) \cdot p(A \mid Y)}{p(B \mid Y)}
  &= \frac{\frac{p(B \cap A \cap Y)}{p(A \cap Y)} \cdot \frac{p(A \cap Y)}{p(Y)}}{\frac{p(B \cap Y)}{p(Y)}}
  \\&= \frac{p(B \cap A \cap Y)}{p(B \cap Y)}
  \\&= p(A \mid B, Y)
\end{align*}\\
Thus, by we have proven that 
$$p(A \mid B, Y) = \frac{p(B \mid A, Y) \cdot p(A \mid Y)}{p(B \mid Y)}$$\\

\noindent
\textbf{(e) Answer:}\\
By (d), we have $$p(A \mid B, Y) = \frac{p(B \mid A, Y) \cdot p(A \mid Y)}{p(B \mid Y)}$$
Now we want to prove that 
$$p(A \mid B, Y) = \frac{p(B \mid A, Y) \cdot p(A \mid Y)}{p(B \mid A, Y) \cdot p(A \mid Y) + p(B \mid \bar{A}, Y) \cdot p(\bar{A} \mid Y)}$$
So we only have to prove that 
$$p(B \mid Y) = p(B \mid A, Y) \cdot p(A \mid Y) + p(B \mid \bar{A}, Y) \cdot p(\bar{A} \mid Y)$$
We start from the right hand side.
\begin{align*}
    p(B \mid A, Y) \cdot p(A \mid Y) + p(B \mid \bar{A}, Y) \cdot p(\bar{A} \mid Y)
    &= \frac{p(B \cap A \cap Y)}{p(A \cap Y)} \cdot \frac{p(A \cap Y)}{p(Y)} + \frac{p(B \cap \bar{A} \cap Y)}{p(\bar{A} \cap Y)} \cdot \frac{p(\bar{A} \cap Y)}{p(Y)}
    \\&= \frac{p(B \cap A \cap Y)}{p(Y)} + \frac{p(B \cap \bar{A} \cap Y)}{p(Y)}
    \\&= \frac{p(B \cap Y)}{p(Y)}
    \\&= p(B \mid Y)
\end{align*}
By the derivation above, we proved the given equation.\\

\noindent
\textbf{(f) Answer:}\\
Let $Y$ be "Baltimore," $A$ be "car is actually blue," and $B$ be "Claimed the car is blue."\\
Then the original equation becomes:\\
\begin{align*}
    p(\text{car is actually blue} \mid \text{Claimed the car is blue}, \text{Baltimore})
    &= \frac{p(B \mid A, Y) \cdot p(A \mid Y)}{p(B \mid A, Y) \cdot p(A \mid Y) + p(B \mid \bar{A}, Y) \cdot p(\bar{A} \mid Y)}
    \\&= \frac{0.8 \times 0.1}{0.8 \times 0.1 + 0.2 \times 0.9}
    \\&= 0.3077
\end{align*}

\noindent
\textbf{Question 3.}\\
\textbf{(a) Answer:}\\
$$\sum_{c \in cry}p(c \mid s) = 1\text{, where } s \in situation$$

\noindent
\textbf{(b) Answer:}\\
\begin{tabular}{ |p{2.5cm}||p{2.5cm}|p{2.5cm}|p{2.5cm}||p{2.5cm}|  }
 \hline
 p(cry, situation) & Predator! & Timber! & I need help! & TOTAL\\
 \hline
 \hline
 bwa   & 0   & 0 & 0.64 & 0.64\\
 \hline
 bwee  & 0   & 0 & 0.08 & 0.08\\
 \hline
 kiki  & 0.2 & 0 & 0.08 & 0.28\\
 \hline
 \hline
 TOTAL & 0.2 & 0 & 0.8 & 1\\
 \hline
\end{tabular}
\\

\noindent
\textbf{(c) Answer:}\\
\textbf{i.} This probability is written as: $p(\text{Predator} \mid \text{kiki})$\\
\textbf{ii.} It can be rewritten without the $\mid$ symbol as: $\frac{p(\text{Predator}, \text{kiki})}{p(\text{kiki})}$\\
\textbf{iii.} Using the above tables, its value is: $\frac{0.2}{0.28} = .7143$\\
\textbf{iv.} Alternatively, Bayes's Theorem allows you to express this probability as:\\
$$\frac{p(\text{kiki} \mid \text{Predator}) \cdot p(\text{Predatory})}{p(\text{kiki} \mid \text{Predator}) \cdot p(\text{Predator}) + p(\text{kiki} \mid \text{Timber}) \cdot p(\text{Timber}) + p(\text{kiki} \mid \text{I need help}) \cdot p(\text{I need help})}$$
\textbf{v.} Using the above tables, the value of this is:\\
$$\frac{1 \times 0.2}{1 \times 0.2 + 0 \times 0 + 0.1 \times 0.8} = 0.7143$$

\noindent
\textbf{Question 4.}\\
\textbf{(a) Answer:}\\
















\end{document}


